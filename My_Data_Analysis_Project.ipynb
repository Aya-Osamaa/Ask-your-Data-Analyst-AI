{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "My-Data-Analysis-Project",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aya-Osamaa/Ask-your-Data-Analyst-AI/blob/main/My_Data_Analysis_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "fCLlyLGQfilr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Minimal Installation (Updated to ensure pyngrok is current)\n",
        "!pip install -q fastapi uvicorn python-multipart openpyxl\n",
        "!pip install -q torch transformers accelerate bitsandbytes\n",
        "!pip install -q pandas plotly\n",
        "!pip install -q langchain_community pydantic\n",
        "!pip install -q pyngrok --upgrade  # <-- ADDED THIS LINE TO FORCE UPGRADE\n",
        "\n",
        "# Re-run all cells after making this change!"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-26T13:49:43.577573Z",
          "iopub.execute_input": "2025-11-26T13:49:43.577797Z",
          "iopub.status.idle": "2025-11-26T13:51:28.449112Z",
          "shell.execute_reply.started": "2025-11-26T13:49:43.577778Z",
          "shell.execute_reply": "2025-11-26T13:51:28.448252Z"
        },
        "id": "PfYwg3SBfilw",
        "outputId": "a601f78a-386a-433f-cc6f-825f5d8a3ba9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.1.0 which is incompatible.\nlangchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load Mistral-7B Safely (Updated for LangChain V0.2+)\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import json\n",
        "import re\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "\n",
        "# Use Mistral-7B with safe loading\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "print(\"Loading Mistral-7B model...\")\n",
        "\n",
        "# Load tokenizer first\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id , use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model with safe settings\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=False,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "print(\"Model loaded! Creating pipeline...\")\n",
        "\n",
        "# Create pipeline\n",
        "mistral_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# Initialize LangChain LLM wrapper\n",
        "llm = HuggingFacePipeline(pipeline=mistral_pipeline)\n",
        "\n",
        "print(\"‚úÖ Mistral-7B and LangChain wrapper loaded successfully!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-26T13:52:06.359795Z",
          "iopub.execute_input": "2025-11-26T13:52:06.360111Z",
          "iopub.status.idle": "2025-11-26T13:53:59.313207Z",
          "shell.execute_reply.started": "2025-11-26T13:52:06.360076Z",
          "shell.execute_reply": "2025-11-26T13:53:59.312352Z"
        },
        "colab": {
          "referenced_widgets": [
            "4402ce3c60b742799a9f032529072b14",
            "59f922d5ed5e47f2b0aadfbee74cf30b",
            "527a12618842401eb64cd6e5cb61c0ae",
            "83be43a50f4c42dfad380b4c5f4f76a5",
            "456dd7165cd444968e59eb6980c687e6",
            "f94cfdcea7d34887840f738d52a2638a",
            "0626460f726b4179b779a76d44bea065",
            "c39e0dd74f2c4dd58d20cf29fde4be0c",
            "ba8592b2230f4315862b61839a841912",
            "d43a8dedacac409fb4ed2b03b0b805c7",
            "f8b613c98a60434bac3c011d770bd88d",
            "61c908629bab4922b501384aee0d3731"
          ]
        },
        "id": "8BenrNU1fily",
        "outputId": "59d0513f-9678-434c-bbe7-f96a6c6d0a30"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-11-26 13:52:24.126734: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764165144.510349      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764165144.678915      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ],
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ],
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ],
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ],
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ],
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error"
        },
        {
          "name": "stdout",
          "text": "Loading Mistral-7B model...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4402ce3c60b742799a9f032529072b14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59f922d5ed5e47f2b0aadfbee74cf30b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "527a12618842401eb64cd6e5cb61c0ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83be43a50f4c42dfad380b4c5f4f76a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "456dd7165cd444968e59eb6980c687e6"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f94cfdcea7d34887840f738d52a2638a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0626460f726b4179b779a76d44bea065"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c39e0dd74f2c4dd58d20cf29fde4be0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba8592b2230f4315862b61839a841912"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d43a8dedacac409fb4ed2b03b0b805c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8b613c98a60434bac3c011d770bd88d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61c908629bab4922b501384aee0d3731"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Model loaded! Creating pipeline...\n‚úÖ Mistral-7B and LangChain wrapper loaded successfully!\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_47/3254283255.py:52: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=mistral_pipeline)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ... (FastAPI and Ngrok imports/setup remain the same) ...\n",
        "from fastapi import FastAPI, UploadFile, File, Form\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import uvicorn\n",
        "import threading\n",
        "import io\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# --- NGROK SETUP ---\n",
        "NGROK_TOKEN = \"35i49MbpyHq3MmlMx8zr50xnJ7Q_VzgqFM423HTEYAuA63Lf\"\n",
        "conf.get_default().auth_token = NGROK_TOKEN\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# --- Define Pydantic Output Schema ---\n",
        "class AnalysisResult(BaseModel):\n",
        "    \"\"\"A clear, well-structured analysis covering key insights and a final conclusion.\"\"\"\n",
        "    explanation: str = Field(description=\"The final, client-facing analysis. MUST NOT contain any code or technical terms. Use Markdown for structure (headings, bolding, lists).\")\n",
        "\n",
        "\n",
        "\n",
        "def create_proper_chart_code(df, query):\n",
        "    \"\"\"\n",
        "    Create properly formatted chart code that works by strictly matching\n",
        "    the query intent to the correct columns (X-axis, Y-axis, aggregation).\n",
        "    \"\"\"\n",
        "\n",
        "    query_lower = query.lower()\n",
        "    df_columns = df.columns.tolist()\n",
        "\n",
        "    # 1. Identify key columns dynamically\n",
        "    y_col = next((col for col in df_columns if col.lower() in ['sales', 'revenue', 'amount']), None)\n",
        "    if not y_col:\n",
        "        y_col = next((col for col in df_columns if col.lower() in ['quantity', 'units']), None)\n",
        "    if not y_col:\n",
        "        numerical_cols = df.select_dtypes(include=['number']).columns\n",
        "        y_col = numerical_cols[0] if len(numerical_cols) > 0 else None\n",
        "    y_metric = y_col if y_col else \"RecordCount\"\n",
        "\n",
        "    # --- A. TOP CATEGORICAL ANALYSIS (Bar/Pie Chart) ---\n",
        "    if any(word in query_lower for word in ['top', 'best', 'most', 'by']) and y_col:\n",
        "        x_col = next((col for col in df_columns if col.lower() in ['category', 'product', 'item']), None)\n",
        "        if not x_col and 'region' in query_lower:\n",
        "            x_col = next((col for col in df_columns if col.lower() in ['region', 'country']), None)\n",
        "\n",
        "        if x_col:\n",
        "            if 'region' in query_lower:\n",
        "                 return f\"\"\"\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "grouped_data = df.groupby('{x_col}')['{y_col}'].sum().reset_index(name='Total_{y_metric}')\n",
        "fig = px.pie(grouped_data, values='Total_{y_metric}', names='{x_col}',\n",
        "             title='{y_metric} Distribution by {x_col}',\n",
        "             hole=0.3)\n",
        "fig.update_traces(textposition='inside', textinfo='percent+label', marker=dict(line=dict(color='#000000', width=1)))\n",
        "fig\n",
        "\"\"\"\n",
        "            else:\n",
        "                return f\"\"\"\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "grouped_data = df.groupby('{x_col}')['{y_col}'].sum().nlargest(10).reset_index(name='Total_{y_metric}')\n",
        "fig = px.bar(grouped_data, x='{x_col}', y='Total_{y_metric}',\n",
        "             title='Top 10 {x_col} by Total {y_metric}',\n",
        "             color='Total_{y_metric}',\n",
        "             color_continuous_scale='Plasma')\n",
        "fig.update_layout(xaxis_tickangle=-45, showlegend=False, yaxis_title='Total {y_metric}')\n",
        "fig.update_traces(texttemplate='%{{y:.2s}}', textposition='outside')\n",
        "fig\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    # --- B. TIME TREND ANALYSIS (Line Chart) ---\n",
        "    date_col = next((col for col in df_columns if 'date' in col.lower()), None)\n",
        "\n",
        "    if date_col and any(word in query_lower for word in ['trend', 'month', 'time', 'year', 'compare', 'difference']):\n",
        "        y_col_final = y_col if y_col else 'OrderCount'\n",
        "\n",
        "        return f\"\"\"\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "df['{date_col}'] = pd.to_datetime(df['{date_col}'])\n",
        "df['Period'] = df['{date_col}'].dt.to_period('M').astype(str)\n",
        "\n",
        "if '{y_col_final}' == 'OrderCount':\n",
        "    monthly_data = df.groupby('Period').size().reset_index(name='OrderCount')\n",
        "else:\n",
        "    monthly_data = df.groupby('Period')['{y_col}'].sum().reset_index(name='Total_{y_metric}')\n",
        "\n",
        "fig = px.line(monthly_data, x='Period', y=monthly_data.columns[-1],\n",
        "              title=f'Monthly Trend for {{monthly_data.columns[-1]}}',\n",
        "              markers=True)\n",
        "fig.update_layout(xaxis_tickangle=-45, yaxis_title=monthly_data.columns[-1])\n",
        "fig\n",
        "\"\"\"\n",
        "\n",
        "    # --- C. GENERIC FALLBACK ---\n",
        "    if y_col:\n",
        "        x_col = next((col for col in df_columns if df[col].dtype == 'object' and len(df[col].unique()) < 50), None)\n",
        "        if x_col:\n",
        "             return f\"\"\"\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "grouped_data = df.groupby('{x_col}')['{y_col}'].sum().reset_index(name='Total_{y_metric}')\n",
        "fig = px.bar(grouped_data, x='{x_col}', y='Total_{y_metric}',\n",
        "             title='Total {y_metric} by {x_col} (Default)',\n",
        "             color='Total_{y_metric}')\n",
        "fig.update_layout(showlegend=False)\n",
        "fig\n",
        "\"\"\"\n",
        "\n",
        "    return \"\"\n",
        "# --- End of Chart Creation Logic ---\n",
        "\n",
        "def analyze_with_mistral(df, query):\n",
        "    \"\"\"Use Mistral-7B wrapped in LangChain for guaranteed analysis and conclusion format.\"\"\"\n",
        "\n",
        "    # 1. Define the Parser\n",
        "    parser = PydanticOutputParser(pydantic_object=AnalysisResult)\n",
        "\n",
        "    # 2. Prepare data context\n",
        "    data_context = f\"\"\"\n",
        "DATASET COLUMNS: {list(df.columns)}\n",
        "- Total rows: {len(df)}\n",
        "- Sample data (first 2 rows):\n",
        "{df.head(2).to_markdown(index=False)}\n",
        "\"\"\"\n",
        "\n",
        "    # 3. Define the Prompt Template\n",
        "    template = f\"\"\"<s>[INST] You are an expert data analyst and business consultant. Your task is to analyze the provided dataset based on the user's question.\n",
        "\n",
        "{data_context}\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "{{format_instructions}}\n",
        "\n",
        "**CRITICAL INSTRUCTION:**\n",
        "1. Your explanation MUST be clean, well-structured, and easy for a non-technical user to read.\n",
        "2. Do NOT include any Python code, SQL, or technical column references in your analysis.\n",
        "3. Provide a clear and concise final conclusion or business recommendation.\n",
        "[/INST]\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=template,\n",
        "        input_variables=[\"data_context\", \"query\"],\n",
        "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        "    )\n",
        "\n",
        "    # 4. Format the final prompt\n",
        "    final_prompt = prompt.format(data_context=data_context, query=query)\n",
        "\n",
        "    try:\n",
        "        # 5. Invoke the LLM through the LangChain wrapper\n",
        "        raw_output = llm.invoke(final_prompt)\n",
        "\n",
        "        # 6. Parse the output (This is where the magic happens, forcing JSON)\n",
        "\n",
        "\n",
        "        json_match = re.search(r'\\{[^{}]*(?:{[^{}]*}[^{}]*)*\\}', raw_output, re.DOTALL)\n",
        "\n",
        "        if json_match:\n",
        "            json_str = json_match.group()\n",
        "            # Clean up the JSON string\n",
        "            json_str = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', json_str).replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "            # Use Pydantic to validate and parse\n",
        "            parsed_data = AnalysisResult.model_validate_json(json_str)\n",
        "            explanation = parsed_data.explanation\n",
        "\n",
        "            chart_code = create_proper_chart_code(df, query)\n",
        "\n",
        "            return {\n",
        "                \"explanation\": explanation,\n",
        "                \"chart_code\": chart_code,\n",
        "                \"success\": True\n",
        "            }\n",
        "        else:\n",
        "            # Fallback if Pydantic parsing fails to find the JSON structure\n",
        "            explanation = f\"Analysis completed, but structured format failed. Raw output: {raw_output}\"\n",
        "            chart_code = create_proper_chart_code(df, query)\n",
        "\n",
        "            return {\n",
        "                \"explanation\": explanation,\n",
        "                \"chart_code\": chart_code,\n",
        "                \"success\": True\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Mistral/LangChain analysis error: {e}\")\n",
        "        # Fallback to the non-LLM based logic\n",
        "        return generate_fallback_analysis(df, query)\n",
        "\n",
        "\n",
        "\n",
        "def generate_fallback_analysis(df, query):\n",
        "    \"\"\"Fallback analysis when Mistral fails \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    query_lower = query.lower()\n",
        "    explanation = \"\"\n",
        "    chart_code = \"\"\n",
        "\n",
        "    try:\n",
        "        if 'category' in query_lower:\n",
        "            if 'Category' in df.columns and 'Quantity' in df.columns:\n",
        "                category_sales = df.groupby('Category')['Quantity'].sum().reset_index()\n",
        "                top_category = category_sales.loc[category_sales['Quantity'].idxmax()]\n",
        "\n",
        "                explanation = f\"üìä **Fallback Analysis:** The top category is **{top_category['Category']}** with **{int(top_category['Quantity']):,}** units sold. The data suggests a concentration of sales in this area.\"\n",
        "\n",
        "        elif 'region' in query_lower and 'Region' in df.columns:\n",
        "            explanation = \"üó∫Ô∏è **Fallback Analysis:** The dataset contains regional data. A pie chart visualization below shows the distribution of sales across different regions.\"\n",
        "\n",
        "        elif any(word in query_lower for word in ['trend', 'month', 'time', 'date']):\n",
        "            explanation = \"üìà **Fallback Analysis:** A time-series analysis is displayed below, showing the trend over time (based on order counts or available numerical columns).\"\n",
        "\n",
        "        else:\n",
        "             explanation = f\"‚ùì **Fallback Analysis:** The system could not run a detailed AI analysis. The dataset has {len(df)} rows and {len(df.columns)} columns. Please see the default chart below based on available columns.\"\n",
        "\n",
        "        chart_code = create_proper_chart_code(df, query)\n",
        "\n",
        "        return {\n",
        "            \"explanation\": explanation,\n",
        "            \"chart_code\": chart_code,\n",
        "            \"success\": True\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        explanation = f\"‚ö†Ô∏è **Critical Fallback Error:** Analysis failed completely. Dataset info: {len(df)} rows, columns: {list(df.columns)}. Error: {e}\"\n",
        "        chart_code = \"\"\n",
        "        return {\n",
        "            \"explanation\": explanation,\n",
        "            \"chart_code\": chart_code,\n",
        "            \"success\": False\n",
        "        }\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "async def analyze_data(file: UploadFile = File(...), query: str = Form(...)):\n",
        "    # ... (file loading and analysis calling logic remains the same) ...\n",
        "    try:\n",
        "        contents = await file.read()\n",
        "\n",
        "        # File loading\n",
        "        df = None\n",
        "        file_extension = file.filename.split('.')[-1].lower()\n",
        "        file_stream = io.BytesIO(contents)\n",
        "\n",
        "        if file_extension in ['csv']:\n",
        "            for enc in [\"utf-8\", \"latin-1\", \"iso-8859-1\"]:\n",
        "                try:\n",
        "                    df = pd.read_csv(file_stream, encoding=enc)\n",
        "                    break\n",
        "                except Exception:\n",
        "                    file_stream.seek(0)\n",
        "                    continue\n",
        "        elif file_extension in ['xlsx', 'xls']:\n",
        "            try:\n",
        "                df = pd.read_excel(file_stream, engine='openpyxl' if file_extension == 'xlsx' else None)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            return {\"error\": \"Failed to load the data file.\"}\n",
        "\n",
        "        # Clean column names\n",
        "        df.columns = df.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True).str.replace(' ', '_')\n",
        "\n",
        "        # Use Mistral for analysis\n",
        "        result = analyze_with_mistral(df, query)\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Server Error: {str(e)}\"}\n",
        "\n",
        "# --- NGROK/UVICORN Server ---\n",
        "def run_server():\n",
        "    ngrok.kill()\n",
        "\n",
        "    import socket\n",
        "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    s.bind(('', 0))\n",
        "    port = s.getsockname()[1]\n",
        "    s.close()\n",
        "\n",
        "    public_url = ngrok.connect(port).public_url\n",
        "    print(f\"\\nüöÄ COPY THIS URL: {public_url}/analyze\\n\")\n",
        "    print(\"‚úÖ Mistral-7B Backend Ready!\")\n",
        "\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
        "\n",
        "# Start server\n",
        "thread = threading.Thread(target=run_server)\n",
        "thread.start()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-26T13:54:02.289853Z",
          "iopub.execute_input": "2025-11-26T13:54:02.2905Z",
          "iopub.status.idle": "2025-11-26T13:54:02.752185Z",
          "shell.execute_reply.started": "2025-11-26T13:54:02.290476Z",
          "shell.execute_reply": "2025-11-26T13:54:02.749576Z"
        },
        "id": "czHYbGp2filz",
        "outputId": "086f1663-cfb7-4391-ec71-9c92290d6c73"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                                                                                                    \nüöÄ COPY THIS URL: https://unpitying-gala-unirritating.ngrok-free.dev/analyze\n\n‚úÖ Mistral-7B Backend Ready!\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "INFO:     Started server process [47]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:58847 (Press CTRL+C to quit)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "haaS2TgJfil1"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}